{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv2:[4.9.0]\n",
      "mediapipe:[0.10.9]\n"
     ]
    }
   ],
   "source": [
    "import mujoco\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from mujoco_parser import MuJoCoParserClass\n",
    "from util import rpy2r\n",
    "np.set_printoptions(precision=2,suppress=True,linewidth=100)\n",
    "plt.rc('xtick',labelsize=6); plt.rc('ytick',labelsize=6)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "print (\"cv2:[%s]\"%(cv2.__version__))\n",
    "print (\"mediapipe:[%s]\"%(mp.__version__))\n",
    "\n",
    "xml_path = '../asset/object/floor_sky.xml'\n",
    "env = MuJoCoParserClass(name='SMPLH-Rig',rel_xml_path=xml_path,VERBOSE=False)\n",
    "\n",
    "env.init_viewer(viewer_title='Joint Pos',viewer_width=1200,viewer_height=800,\n",
    "                viewer_hide_menus=True)\n",
    "env.update_viewer(azimuth=180,distance=1.0,elevation=-21,lookat=[0.02,-0.03,0.8])\n",
    "env.reset()\n",
    "\n",
    "hand_joint_idx_pairs = [[0,1],[1,2],[2,3],[3,4],\n",
    "                        [0,5],[5,6],[6,7],[7,8],\n",
    "                        [5,9],[9,10],[10,11],[11,12],\n",
    "                        [9,13],[13,14],[14,15],[15,16],\n",
    "                        [0,17],[13,17],[17,18],[18,19],[19,20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: unable to open video source:  1\n",
      "Warning: unable to open video source:  2\n",
      "Warning: unable to open video source:  3\n",
      "Warning: unable to open video source:  4\n",
      "Warning: unable to open video source:  5\n",
      "Warning: unable to open video source:  6\n",
      "Warning: unable to open video source:  7\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: out device of bound (0-0): 1\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 2\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 3\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 4\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 5\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 6\n",
      "OpenCV: camera failed to properly initialize!\n",
      "OpenCV: out device of bound (0-0): 7\n",
      "OpenCV: camera failed to properly initialize!\n"
     ]
    }
   ],
   "source": [
    "valid_cams = []\n",
    "for i in range(8):\n",
    "    cap = cv2.VideoCapture(i)\n",
    "    if cap is None or not cap.isOpened():\n",
    "        print('Warning: unable to open video source: ', i)\n",
    "    else:\n",
    "        valid_cams.append(i)\n",
    "\n",
    "print(valid_cams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711017722.632069       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M2 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize MediaPipe hand model\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode        = False,\n",
    "    max_num_hands            = 2,\n",
    "    model_complexity         = 1,\n",
    "    min_detection_confidence = 0.25,\n",
    "    min_tracking_confidence  = 0.25,\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    img.flags.writeable = False\n",
    "    res = hands.process(img)\n",
    "\n",
    "    img_with_res = img.copy()\n",
    "    if res.multi_hand_landmarks:\n",
    "        for hand_landmarks in res.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                img_with_res,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),\n",
    "            )\n",
    "\n",
    "    n_landmark = 21 # fixed\n",
    "    if res.multi_hand_world_landmarks: # if hands are detected\n",
    "        n_detected_hand = len(res.multi_hand_world_landmarks) # number of detected hand(s)\n",
    "        joint_xyzs_list = []\n",
    "        for h_idx in range(n_detected_hand):\n",
    "            joint_xyzs = np.zeros((n_landmark,3)) # [21 x 3]\n",
    "            for l_idx in range(n_landmark): # for each landmark\n",
    "                joint_xyzs[l_idx,0] = res.multi_hand_world_landmarks[h_idx].landmark[l_idx].x\n",
    "                joint_xyzs[l_idx,1] = res.multi_hand_world_landmarks[h_idx].landmark[l_idx].y\n",
    "                joint_xyzs[l_idx,2] = res.multi_hand_world_landmarks[h_idx].landmark[l_idx].z\n",
    "            joint_xyzs_list.append(joint_xyzs)\n",
    "    \n",
    "    else: # if no hands are detected\n",
    "        n_detected_hand = 0\n",
    "        joint_xyzs_list = []\n",
    "\n",
    "    # cv2.imshow('original_video', img)\n",
    "    cv2.imshow('hands_3d', img_with_res)\n",
    "    \n",
    "    # Hand rotation offset\n",
    "    R = rpy2r(np.radians([0,0,-90]))@rpy2r(np.radians([0,90,0]))\n",
    "    # Plot hand joints\n",
    "    colors = [(1,0,0,1),(0,0,1,1),(0,0,0,0)]\n",
    "    xyz_offsets = [np.array([0,-0.25,1.0]),np.array([0,0.25,1.0]),np.array([0,0,0])]\n",
    "    for h_idx in range(n_detected_hand): # for each hand\n",
    "        joint_xyzs = joint_xyzs_list[h_idx]@R # ndarray [21 x 3]\n",
    "        n_joint = 21\n",
    "        for j_idx in range(n_joint):\n",
    "            joint_xyz = joint_xyzs[j_idx,:]+xyz_offsets[h_idx] # [3]\n",
    "            rgba = colors[h_idx]\n",
    "            env.plot_sphere(p=joint_xyz,r=0.005,rgba=rgba)\n",
    "\n",
    "        for hand_joint_idx_pair in hand_joint_idx_pairs:\n",
    "            idx_fr,idx_to = hand_joint_idx_pair[0],hand_joint_idx_pair[1]\n",
    "            env.plot_cylinder_fr2to(\n",
    "                p_fr = joint_xyzs[idx_fr,:]+xyz_offsets[h_idx],\n",
    "                p_to = joint_xyzs[idx_to,:]+xyz_offsets[h_idx],\n",
    "                r    = 0.003,\n",
    "                rgba = [0.7,0.7,0.7,1],\n",
    "            )\n",
    "\n",
    "    env.plot_T(p=np.zeros(3),R=np.eye(3,3),PLOT_AXIS=True,axis_len=0.1,axis_width=0.005)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    if key==27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "env.close_viewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online_generation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
